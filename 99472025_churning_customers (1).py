# -*- coding: utf-8 -*-
"""99472025_Churning_Customers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RCS04aG00cc5xPhoEfuoutfkuRhwhclZ
"""

from google.colab import drive
drive.mount('/content/drive')

# Import statements
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectFromModel
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler,OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, concatenate
from tensorflow.keras.optimizers import Adam
import seaborn as sns
import matplotlib.pyplot as plt

data= pd.read_csv("/content/drive/My Drive/Colab Notebooks/CustomerChurn_dataset.csv")

data.info()

data.describe()

churn_data = data.copy()

churn_data

churn_data = churn_data.drop('customerID', axis=1)
print(churn_data)

# Convert 'total_charges' column to numeric (integer)
churn_data['TotalCharges'] = pd.to_numeric(churn_data['TotalCharges'], errors='coerce')
churn_data['TotalCharges'].fillna(0, inplace=True)
churn_data['TotalCharges'] = churn_data['TotalCharges'].astype(int)

churn_data.info()

numeric_columns = churn_data.select_dtypes(include=['number'])
categorical_columns = churn_data.select_dtypes(include=['object'])

print("\nNumeric Columns:")
print(numeric_columns)

print("\nCategorical Columns:")
label_encoder = LabelEncoder()
print(label_encoder)

copied_data= data.copy()
for column in categorical_columns:
       copied_data[column] = label_encoder.fit_transform(data[column])

for column in numeric_columns:
       copied_data[column] = label_encoder.fit_transform(data[column])

copied_data

print(copied_data)

print(data)

copied_data.info()

copied_data= copied_data.drop('customerID',axis=1)

copied_data.info()

copied_data.dtypes



X = copied_data.drop('Churn', axis=1)
y = copied_data['Churn']
scaler= StandardScaler()
X_scaled= scaler.fit_transform(X)
copied_data

# Instantiate the model
rf_model = RandomForestClassifier()

# Fit the model to your data
rf_model.fit(X, y)

# Get feature importances
feature_importances = rf_model.feature_importances_

# Create a DataFrame with feature names and their importance scores
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})

# Sort the features by importance in descending order
top_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(5)

# Display the top features
print(top_features)

# Plot feature importances
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=top_features, orient='h')
plt.title('Top 5 Features for Churn Prediction')
plt.show()

sns.boxplot(x='Churn', y='TotalCharges' , data=copied_data)
plt.show()

# Example: Visualize the relationship between MonthlyCharges and Churn
sns.boxplot(x='Churn', y='MonthlyCharges', data=copied_data)
plt.show()

sns.boxplot(x='Churn', y='tenure' , data=copied_data)
plt.show()

sns.boxplot(x='Churn', y='Contract' , data=copied_data)
plt.show()

sns.boxplot(x='Churn', y='PaymentMethod' , data=copied_data)
plt.show()

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)



import keras
from keras.models import Model
from keras.layers import Input, Dense
from keras.optimizers import Adam
from keras.utils import to_categorical

# Keras Functional API model
input_layer = Input(shape=(X_train.shape[1],))
hidden_layer_1 = Dense(32, activation='relu')(input_layer)
hidden_layer_2 = Dense(24, activation='relu')(hidden_layer_1)
hidden_layer_3 = Dense(12, activation='relu')(hidden_layer_2)
output_layer = Dense(1, activation='sigmoid')(hidden_layer_3)

model = Model(inputs=input_layer, outputs=output_layer)

model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])


model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))

_, accuracy = model.evaluate(X_train, y_train)
accuracy*100
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy*100:.4f}')

!pip install keras-tuner

!pip install scikeras

from scikeras.wrappers import KerasClassifier

from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.metrics import make_scorer
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import make_pipeline

# Define the parameter grid for grid search
param_grid = {
    'mlpclassifier__activation': ['relu', 'tanh', 'logistic'],
    'mlpclassifier__alpha': [0.0001, 0.001, 0.01],
    'mlpclassifier__hidden_layer_sizes': [(50,), (100,), (50, 50)],
}

# Use StratifiedKFold for classification tasks
cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

# Create MLP model with the best hyperparameters
mlp_model = MLPClassifier(
    activation='relu',
    alpha=0.0001,
    hidden_layer_sizes=(50,),
    random_state=42
)

# Create a pipeline with preprocessing and MLP model
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # Assuming all features are numeric
    ('mlpclassifier', mlp_model)
])

# Create GridSearchCV instance
grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring='accuracy', cv=cv, refit='auc', verbose=1)

# Perform grid search
grid_result = grid_search.fit(X, y)

# Display the best parameters and the corresponding AUC-ROC score
print("Best AUC-ROC: %f using %s" % (grid_result.best_score_, grid_result.best_params_))

# Access the best model and its parameters
best_model = grid_search.best_estimator_
best_params = grid_search.best_params_

y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

best_params

print(f'Test Accuracy: {accuracy*100}')
print(f'Best Hyperparameters: {best_params}')

from sklearn.model_selection import GridSearchCV, train_test_split

from sklearn.pipeline import make_pipeline

# Assuming X and y are your features and labels
# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocessing using StandardScaler
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), X.columns.tolist())
    ],
    remainder='passthrough'
)

# Create MLP model with the best hyperparameters
mlp_model = MLPClassifier(
    activation='relu',
    alpha=0.0001,
    hidden_layer_sizes=(32,),
    learning_rate='adaptive',
    solver='adam',
    random_state=42



)

# Create a pipeline with preprocessing and MLP model
pipeline = make_pipeline(preprocessor, mlp_model)

pipeline.fit(X_train, y_train)

# Evaluate the model on the test set
y_pred = pipeline.predict(X_test)

# Calculate and print the accuracy score
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy*100}')

from sklearn.metrics import accuracy_score, roc_auc_score

# Evaluate the model on the test set
y_pred_proba = best_model.predict_proba(X_test)[:, 1]

# Convert probabilities to binary predictions using a threshold
threshold = 0.5
y_pred_binary = (y_pred_proba > threshold).astype(int)

# Calculate and print the accuracy score
accuracy = accuracy_score(y_test, y_pred_binary)
print(f'Accuracy: {accuracy*100}')

# Calculate and print the AUC score
auc_score = roc_auc_score(y_test, y_pred_proba)
print(f'AUC Score: {auc_score*100}')

!pip install joblib

import joblib

model_filename = 'churning_customer_model.joblib'

joblib.dump(best_model, 'churning_customer_model.joblib')

print(f'Model saved to {"churning_customer_model.joblib"}')

